<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://yihao001.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://yihao001.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-20T01:22:38+00:00</updated><id>https://yihao001.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Evaluating XAI algorithms</title><link href="https://yihao001.github.io/blog/2024/reconfirm/" rel="alternate" type="text/html" title="Evaluating XAI algorithms"/><published>2024-07-13T07:30:11+00:00</published><updated>2024-07-13T07:30:11+00:00</updated><id>https://yihao001.github.io/blog/2024/reconfirm</id><content type="html" xml:base="https://yihao001.github.io/blog/2024/reconfirm/"><![CDATA[<p>tl;dr: We propose RE-CONFIRM, a framework for evaluating explanations produced by XAI algorithms in an objective manner.</p> <ul> <li>RE-CONFIRM comprises 8 metrics that encompasses various aspects of a typical ML pipeline (data, models, labels).</li> <li>Using this framework on 2 fMRI datasets, we found that the combination of GAT and GNNExplainer produced explanations that are relatively more robust than other combinations considered in our study.</li> <li>Future biomarker discovery studies (that are based on applying explainers to deep learning models) can use RE-CONFIRM to determine the robustness of their model’s explanations.</li> </ul> <p>Edit: Here’s is the <a href="https://openreview.net/pdf?id=3kti62n63m">link</a> to our paper, which includes an additional set of experiments on top of those presented here. More details will be shared soon. :)</p> <h2 id="problem">Problem</h2> <p>Numerous explainable AI algorithms have been developed and applied on biological datasets to derive potential biomarkers (i.e. salient features unique to a certain disorder/trait). While they can compute a score for each input feature to indicate its importance for the task (e.g. healthy subject vs patient with disorders), some studies have shown that these scores do not consistently produce explanations that are sensible for the task <d-cite key="adebayo2018sanity"></d-cite>.</p> <p>Furthermore, there are few tools available to evaluate the robustness of these scores. Over the past years, model explainability has evolved into an essential component of recent research papers on using machine learning algorithms on biomedical datasets. While many papers are focused on creating deep / machine learning techniques that surpass the state-of-the-art, showing that the proposed model performs better (e.g. higher accuracy, lower error) is no longer sufficient - it is often expected for these studies to also highlight the salient features that the model identified to be important for performing the task.</p> <p>Unfortunately, such analyses are often done at surface level, limited to highlighting the top 10 features, followed by rather limited cross-referencing with existing literature (e.g. mentioning that a subset of these top features were mentioned in some previous research paper, but often neglecting those that are not). One reason for this is the lack of standardised checklists or metrics for evaluating the robustness of the explanations produced by their proposed model<d-footnote> There are some studies that made the effort to use multiple explainers, e.g. <d-cite key="gallo2023functional"></d-cite></d-footnote>. <strong>Thus, we need better ways to evaluate these explainable AI algorithms (<em>explainers</em>).</strong></p> <h2 id="existing-solutions">Existing solutions</h2> <p>Existing tools to evaluate the robustness of explainers typically came from research that highlighted issues with existing explainers. For example, Adebayo et al. <d-cite key="adebayo2018sanity"></d-cite> found that even after randomising the weights of hidden layers, some explainers still produced very similar explanations (heatmaps). This shows that there are explainers that are unexpectedly invariant to model weights. From this study, model parameter randomisation check was developed and became one of the first techniques to evaluate explainers.</p> <p>Since then, several tests have been developed that encompasses various aspects of a typical machine learning model (e.g. data, model weights, labels). These can be categorised into the following categories as proposed by Nauta et al. <d-cite key="nauta2023anecdotal"></d-cite>:</p> <ol> <li>Correctness: Whether the explanation is faithful to the model ; e.g. <strong>Model Parameter Randomisation Check (MPRC)</strong> - if the explanation remains the same even when the model weights are randomised, then it is not considered to be faithfully correct.</li> <li>Consistency: Whether explainers are deterministic + models with same inputs and outputs should give the same explanation ; e.g. <strong>Implementation Invariance (II)</strong> - models with same architecture but different initialisation (seed) should have very similar explanations.</li> <li>Completeness: Whether the explanation captures the entire model behaviour ; e.g. <strong>Fidelity+</strong> - retain a subset of the most important features, train another model on this subset and check whether the change in prediction is small.</li> <li>Continuity: Small changes in input that does not change the output much should not affect the explanation greatly too ; e.g. <strong>Stability</strong> measures the changes in model explanations when perturbations are introduced to the input.</li> <li>Contrastivity: Whether the explanation is discriminative with respect to targets; e.g. <strong>Data Randomisation Check (DRC)</strong> - a model trained on a dataset with shuffled labels should have very different explanations ; <strong>Sensitivity</strong> - explanations for each class should be distinct.</li> </ol> <p>These categories are a subset of the full Co-12 framework they proposed. Other categories such as compactness, coherence and controllability are not considered as they are less relevant for biomarker discovery.</p> <h2 id="methods">Methods</h2> <p>In this study, we attempt to make progress on developing a set of metrics that could serve as a standardised checklist for future research that applies ML to biological datasets.</p> <p>We focus on two open source datasets: <a href="http://preprocessed-connectomes-project.org/abide/">ABIDE</a> and <a href="http://preprocessed-connectomes-project.org/adhd200/">ADHD-200</a>. These datasets contain over 700 functional magnetic resonance imaging (fMRI) scans along with phenotypic data (demographics, clinical test scores, presence/absence of disorders). Functional connectivity matrices are often derived from them and used as inputs to machine learning models.</p> <p>We use a subset of Nauta’s Co-12 framework as discussed above. These existing evaluation metrics are rather generic. Thus, we additionally proposed 2 novel metrics that capture topological properties characteristic of brain functional connectomes. More information about them will be revealed when the paper is published.</p> <p>Put together, these 8 metrics form the basis of our proposed RE-CONFIRM framework.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog_reconfirm-480.webp 480w,/assets/img/blog_reconfirm-800.webp 800w,/assets/img/blog_reconfirm-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog_reconfirm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="RE-CONFIRM" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> RE-CONFIRM comprises 8 metrics that evaluate the robustness of the saliency scores produced by explainers. </div> <p>In our implementation of the metrics, we measured the changes in explanations quantitatively using Hellinger Distance (for MPRC, DRC, Stability, Sens, II). For Fidelity, we focused on the top 20 features. To evaluate stability, we perturbed the input graph data by rewiring edges between second-degree nodes <d-cite key="agarwal2023evaluating"></d-cite> and introducing Gaussian noise to node features. We then measured the similarity between explanations using cosine similarity.</p> <h2 id="results">Results</h2> <p>We experimented with 3 predictors and 4 explainers, namely:</p> <p><strong>Predictors</strong>: (i) ChebGCN, (ii) GraphSAGE, (iii) Graph Attention Network (GAT). 2 hidden layers were used for all models.</p> <p><strong>Explainers</strong>: (i) Integrated Gradients (IG), (ii) Guided Backpropagation (GBP), (iii) GNNExplainer (GNNExp), (iv) Attention.</p> <p>Saliency scores can be influenced by a multitude of factors: (i) the base model, (ii) the explainer and (iii) the dataset. Thus, 3 sets of experiments were performed to evaluate (i) Performance across predictors, (ii) Performance across explainers, (iii) Performance across datasets.</p> <details><summary>Click here to view the result tables</summary> <table> <thead> <tr> <th> </th> <th>ABIDE</th> <th> </th> <th> </th> <th> </th> <th>ADHD</th> <th> </th> <th> </th> <th> </th> </tr> </thead> <tbody> <tr> <td>Metrics</td> <td>GAT</td> <td>SAGE</td> <td>GCN</td> <td>Cheb</td> <td>GAT</td> <td>SAGE</td> <td>GCN</td> <td>Cheb</td> </tr> <tr> <td>MPRC (↑)</td> <td><strong>0.488</strong></td> <td>0.372</td> <td>0.162</td> <td>0.379</td> <td>0.300</td> <td>0.248</td> <td><strong>0.411</strong></td> <td>0.207</td> </tr> <tr> <td>DRC (↑)</td> <td><strong>0.235</strong></td> <td>0.123</td> <td>0.177</td> <td>0.152</td> <td><strong>0.241</strong></td> <td>0.162</td> <td>0.156</td> <td>0.137</td> </tr> <tr> <td>Fidelity+ (↑)</td> <td><strong>0.330</strong></td> <td>0.044</td> <td>0.023</td> <td>0.092</td> <td>0.292</td> <td>0.202</td> <td><strong>0.349</strong></td> <td>0.237</td> </tr> <tr> <td>Stability (↑)</td> <td>1.000</td> <td>1.000</td> <td>1.000</td> <td>1.000</td> <td>1.000</td> <td>1.000</td> <td>1.000</td> <td>1.000</td> </tr> <tr> <td>Fidelity- ( ↓ )</td> <td>0.101</td> <td>0.008</td> <td><strong>0.006</strong></td> <td>0.031</td> <td>0.113</td> <td><strong>0.075</strong></td> <td>0.102</td> <td>0.099</td> </tr> <tr> <td>Sens ( ↓ )</td> <td>0.011</td> <td><strong>0.008</strong></td> <td>0.010</td> <td>0.008</td> <td>0.009</td> <td>0.011</td> <td>0.011</td> <td><strong>0.004</strong></td> </tr> <tr> <td>II ( ↓ )</td> <td><strong>0.009</strong></td> <td>0.030</td> <td>0.022</td> <td>0.029</td> <td>0.015</td> <td>0.021</td> <td><strong>0.011</strong></td> <td>0.021</td> </tr> </tbody> </table> <p>Table 1. Evaluation metrics for GNNExplainer on both datasets (whole).</p> <table> <thead> <tr> <th> </th> <th>GNNExp</th> <th> </th> <th>GBP</th> <th> </th> <th> </th> <th> </th> <th>IG</th> <th> </th> <th> </th> <th> </th> <th>Attn</th> </tr> </thead> <tbody> <tr> <td>Metric</td> <td>GAT</td> <td>GCN</td> <td>GAT</td> <td>SAGE</td> <td>GCN</td> <td>Cheb</td> <td>GAT</td> <td>SAGE</td> <td>GCN</td> <td>Cheb</td> <td>GAT</td> </tr> <tr> <td>MPRC (↑)</td> <td>0.300</td> <td><strong>0.411</strong></td> <td>0.316</td> <td>0.137</td> <td>0.278</td> <td>0.137</td> <td>0.086</td> <td>0.144</td> <td>0.365</td> <td>0.316</td> <td>0.166</td> </tr> <tr> <td>DRC (↑)</td> <td><strong>0.241</strong></td> <td>0.156</td> <td>0.270</td> <td>0.132</td> <td>0.174</td> <td>0.118</td> <td>0.179</td> <td>0.196</td> <td>0.194</td> <td>0.605</td> <td>0.160</td> </tr> <tr> <td>Fidelity+ (↑)</td> <td>0.292</td> <td><strong>0.349</strong></td> <td>0.197</td> <td>0.135</td> <td>0.152</td> <td>0.168</td> <td>0.287</td> <td>0.181</td> <td>0.212</td> <td>0.179</td> <td>0.127</td> </tr> <tr> <td>Stability (↑)</td> <td><strong>1.000</strong></td> <td><strong>1.000</strong></td> <td>0.999</td> <td><strong>1.000</strong></td> <td><strong>1.000</strong></td> <td><strong>1.000</strong></td> <td>0.452</td> <td>0.697</td> <td>0.728</td> <td>0.793</td> <td>0.847</td> </tr> <tr> <td>Fidelity- (↓)</td> <td>0.113</td> <td><strong>0.102</strong></td> <td>0.133</td> <td>0.104</td> <td>0.122</td> <td>0.142</td> <td>0.248</td> <td>0.200</td> <td>0.178</td> <td>0.150</td> <td>0.117</td> </tr> <tr> <td>Sens (↓)</td> <td><strong>0.009</strong></td> <td>0.011</td> <td>0.008</td> <td>0.009</td> <td>0.008</td> <td>0.007</td> <td>0.038</td> <td>0.022</td> <td>0.024</td> <td>0.009</td> <td>0.084</td> </tr> <tr> <td>II (↓)</td> <td>0.015</td> <td>0.011</td> <td>0.015</td> <td>0.010</td> <td>0.008</td> <td>0.009</td> <td>0.016</td> <td><strong>0.007</strong></td> <td>0.017</td> <td>0.013</td> <td>0.138</td> </tr> </tbody> </table> <p>Table 2. Evaluation metrics for GNNExplainer, GBP and IG on ADHD-200 (whole).</p> <table> <thead> <tr> <th> </th> <th>ABIDE</th> <th> </th> <th> </th> <th> </th> <th>ADHD</th> <th> </th> <th> </th> <th> </th> </tr> </thead> <tbody> <tr> <td>Metric</td> <td>GAT</td> <td>SAGE</td> <td>GCN</td> <td>Cheb</td> <td>GAT</td> <td>SAGE</td> <td>GCN</td> <td>Cheb</td> </tr> <tr> <td>MPRC (↑)</td> <td>0.322</td> <td><strong>0.564</strong></td> <td>0.169</td> <td>0.144</td> <td>0.282</td> <td>0.319</td> <td><strong>0.423</strong></td> <td>0.166</td> </tr> <tr> <td>DRC (↑)</td> <td><strong>0.233</strong></td> <td>0.160</td> <td>0.197</td> <td>0.153</td> <td>0.193</td> <td>0.086</td> <td><strong>0.168</strong></td> <td>0.168</td> </tr> <tr> <td>Fidelity+ (↑)</td> <td>0.250</td> <td><strong>0.260</strong></td> <td>0.037</td> <td>0.228</td> <td><strong>0.413</strong></td> <td>0.073</td> <td>0.300</td> <td>0.035</td> </tr> <tr> <td>Stability (↑)</td> <td>1.000</td> <td>1.000</td> <td>1.000</td> <td>1.000</td> <td>1.000</td> <td>1.000</td> <td>1.000</td> <td>1.000</td> </tr> <tr> <td>Fidelity- (↓)</td> <td>0.052</td> <td>0.054</td> <td><strong>0.011</strong></td> <td>0.069</td> <td>0.109</td> <td>0.035</td> <td>0.097</td> <td><strong>0.011</strong></td> </tr> <tr> <td>Sens (↓)</td> <td>0.023</td> <td><strong>0.009</strong></td> <td>0.011</td> <td>0.012</td> <td>0.010</td> <td><strong>0.006</strong></td> <td>0.015</td> <td>0.011</td> </tr> <tr> <td>II (↓)</td> <td>0.012</td> <td><strong>0.009</strong></td> <td>0.017</td> <td>0.013</td> <td>0.011</td> <td>0.011</td> <td>0.012</td> <td><strong>0.009</strong></td> </tr> </tbody> </table> <p>Table 3. Metrics for GNNExplainer on both datasets (site NYU only).</p> </details> <p><strong>Performance across predictors</strong>: We found that GAT performs the best, with much higher MPRC, DRC and Fidelity+ than other models. GCN was found to be relatively erratic as it has the highest MPRC and Fidelity+ for ADHD, but lowest in ABIDE. Overall, positive findings for all models include low Fidelity-, very high stability and low II, However, all models have low DRC, Fidelity+ and target sensitivity.</p> <p><strong>Performance across explainers</strong>: GNNExplainer generally performs the best across most metrics and this finding is also replicated in ABIDE. Attention explainer and IG have much lower stability and MR than others, while GBP has lower MPRC. Notably, the previous finding of low Fidelity- and II remains, but all explainers still have low DRC, Fidelity+ (except GNNExplainer) and target sensitivity.</p> <p><strong>Performance across datasets</strong>: Comparing the results from the whole dataset, it is evident that the metrics are sensitive to dataset size<d-footnote>Site NYU, ABIDE (N=126) ; Site NYU, ADHD-200 (N=396)</d-footnote>. While the combination of GNNExplainer and GAT remains generally superior, GraphSAGE (for ABIDE) and GCN (for ADHD) has the highest MPRC, with a higher value than the whole dataset scenario. Again, DRC, Fidelity-, II and target sensitivity remain low.</p> <h2 id="discussion">Discussion</h2> <p>Key findings of our experiments include:</p> <ul> <li>GNNExplainer and GAT exhibits greatest robustness amongst all combinations.</li> <li>Low Fidelity- and II across all combinations suggests that unimportant features are correctly given low scores and the scores are consistent.</li> <li>Low target sensitivity and DRC indicates poor contrastivity, raising concerns about whether the saliency features identified for the disease class are actually unique to them (as expected from a strong biomarker)</li> <li>GBP was found to have generally low MPRC, which aligns with findings in Adebayo et al. that GBP is invariant to their randomisation tests in computer vision datasets.</li> <li>IG and attention explainer have consistently lower stability, suggesting that they might not be a good choice for biomarker discovery.</li> </ul> <p>Future work in this research direction could consider that:</p> <ul> <li>Our study is limited to explaining the model’s decision ; alternatively, one could further study the ‘phenomenon’ (i.e. use the ground truth labels as the target instead of the model’s predictions) <d-cite key="amara2022graphframex"></d-cite> to corroborate these findings.</li> <li>Our study is focused on post-hoc gradient-based explainers<d-footnote>Deep neural networks are often seen as black boxes - most of them do not provide an intuitive understanding of how they arrive at a decision (in contrast to decision trees, where the criteria used to split the dataset is clearly represented in each node). Post-hoc explainers are limited to highlighting input-output relationships and do not give insight to the models beyond that (unlike decision trees). However, for the purpose of biomarker discovery, identifying the most salient features would be sufficient for the task.</d-footnote>. Perturbation-based approaches and other intrinsically interpretable models (e.g. graph pooling) could be studied in future.</li> </ul> <h2 id="code">Code</h2> <p>Our code repository will be released soon!</p> <p>We hope that RE-CONFIRM will serve as the first step towards a standard for quantitative evaluation of salient features detected by future studies.</p>]]></content><author><name></name></author><category term="acad"/><category term="XAI,"/><category term="fmri,"/><category term="ml"/><summary type="html"><![CDATA[Measuring robustness of explanations objectively]]></summary></entry><entry><title type="html">Revamp</title><link href="https://yihao001.github.io/blog/2024/revamp/" rel="alternate" type="text/html" title="Revamp"/><published>2024-07-03T13:40:16+00:00</published><updated>2024-07-03T13:40:16+00:00</updated><id>https://yihao001.github.io/blog/2024/revamp</id><content type="html" xml:base="https://yihao001.github.io/blog/2024/revamp/"><![CDATA[<p>hi!</p> <p>I’ve finally switched to a new layout with dark mode and the website is now so much easier on the eyes.</p> <p>This is so much more flexible and it even has distil-like posts which I’ll definitely use some time in the future.</p>]]></content><author><name></name></author><category term="non-acad"/><category term="aside"/><summary type="html"><![CDATA[refresh!]]></summary></entry><entry><title type="html">Confusion Matrices in sklearn</title><link href="https://yihao001.github.io/blog/2020/confusion/" rel="alternate" type="text/html" title="Confusion Matrices in sklearn"/><published>2020-05-21T16:00:00+00:00</published><updated>2020-05-21T16:00:00+00:00</updated><id>https://yihao001.github.io/blog/2020/confusion</id><content type="html" xml:base="https://yihao001.github.io/blog/2020/confusion/"><![CDATA[<p>The confusion_matrix function in sklearn is useful but the documentation can be confusing.</p> <p>Assuming that you’re using integer labels (i.e. 0 or 1), one’d think that True Positive will be on the top left corner, but it’s on the diagonally opposite end instead!</p> <p>The official documentation doesn’t directly provide 2 critical pieces of information:</p> <ol> <li>A figure would’ve helped, so here it is (for binary classification):</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/confusion_matrix-480.webp 480w,/assets/img/confusion_matrix-800.webp 800w,/assets/img/confusion_matrix-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/confusion_matrix.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ol> <li> <p>What is considered positive or negative by default? (i.e. without feeding in the ‘labels’ param)</p> <p>0 is assumed to be negative (N), 1 is thus positive (P)</p> </li> </ol> <p>If your labels are strings, the above <strong>does not always apply</strong>. Instead, sklearn will follow the <strong>alphabetical order</strong> of the labels: e.g. if the labels are ‘Alice’ and ‘Bob’, [0,0] is the number of instances where prediction == ground truth == ‘Alice’. [0,1], i.e. top right cell, is the number of times ‘Bob’ was predicted but ground truth is ‘Alice’.</p>]]></content><author><name></name></author><category term="acad"/><category term="ml"/><summary type="html"><![CDATA[confusion matrices are confusing]]></summary></entry><entry><title type="html">Learning Machines</title><link href="https://yihao001.github.io/blog/2020/learning/" rel="alternate" type="text/html" title="Learning Machines"/><published>2020-03-21T16:00:00+00:00</published><updated>2020-03-21T16:00:00+00:00</updated><id>https://yihao001.github.io/blog/2020/learning</id><content type="html" xml:base="https://yihao001.github.io/blog/2020/learning/"><![CDATA[<p>Attended <a href="https://twitter.com/NTUsg/status/1215818307460157440">a sharing session by GIC’s CEO</a> and even though it’s a part of a <code class="language-plaintext highlighter-rouge">Adaptive Skills 4.0 Speaker Series</code> and sounds only tangentially related to research / academia, it helped to give some structure to the work I’m doing.</p> <p>3 key takeaways (based on my own interpretation):</p> <ol> <li> <p>Being a learning machine</p> <p>It’s always good to have condensed and pre-processed information, like how a country’s leaders can get the best specialists in the area to explain to them about concept they might not know. Most of us aren’t world leaders (yet, haha!), but we can similarly maintain a good cache / database of past knowledge (that’s quickly and easily retrievable) + know people in adjacent fields who we can consult (and of course, you should help them out too). Most importantly, be in a constant process of learning (observe, think, express/apply) - in other words, don’t just be a database. Act on the information you have.</p> </li> <li> <p>Problem stages: Business vs Engineering vs Science</p> <p>In academia, we’re mostly working on ‘Science’ problems and we’re in the business of creating new technology. When the ‘Science’ problems are solved / proven to be possible, then it’s a matter of engineering the right solution. Finally, if / when the engineering problems are addressed, it doesn’t mean that it will lead to a viable business. Perhaps the engineering problem is solved, but the product might not be at a viable cost that consumers are willing to pay a premium over it.</p> <p>E.g. Selling a bicycle: someone had to figure out the physics of how a bicycle can operate (e.g. friction between wheels and ground, circular motion, torque); then comes the engineering problem (e.g. aerodynamics) ; then it’s a matter of developing a business model that can ensure sustainability of the business (e.g. cost management, procurement, marketing)</p> <p>All companies in all industries are in some stage of this process. Some industries are not yet at the business stage, or even the engineering stage (e.g. fMRI). Blockchain is an area where lots of work is still going on in terms of engineering (the science problem is solved to an extent, but some people have tried to rush the business side of things too - that’s possibly why we see the crazy swings it has when more people realize that some of the security issues are not ironed out yet!</p> <p>Be in time for the future - not too early.</p> </li> <li> <p>Investing in such a turbulent environment</p> <ul> <li>Besides think about what to change, what do you not change? Some organisations have to stick to principles, even in crisis.</li> <li>Diversification is still the only free lunch, but you need more than it to do better</li> <li>Take risks that you understand + you’re compensated appropriately</li> </ul> </li> </ol>]]></content><author><name></name></author><category term="non-acad"/><category term="aside"/><summary type="html"><![CDATA[beyond machine learning]]></summary></entry><entry><title type="html">Decomposing huge matrices</title><link href="https://yihao001.github.io/blog/2020/decompose/" rel="alternate" type="text/html" title="Decomposing huge matrices"/><published>2020-01-09T16:00:00+00:00</published><updated>2020-01-09T16:00:00+00:00</updated><id>https://yihao001.github.io/blog/2020/decompose</id><content type="html" xml:base="https://yihao001.github.io/blog/2020/decompose/"><![CDATA[<p>In fMRI analyses, a common approach is to use a correlation matrix to study the relationships between various brain regions. Brain parcellations schemes seem to be getting finer over the years and that will increase with more detailed parcellation schemes proposed / better imaging resolution - that means there might be more and more regions.</p> <p>One such atlas is from Power et al., which gives 264 regions of interest. A 264 x 264 correlation matrix gives 34716 unique features (correlation between the time series of each pair of nodes). If you want to study the correlation between these edge features (across subjects) - that’s going to become a 34716 x 34716 matrix!</p> <p>Processing it is going to be a pain. There are some tricks mentioned below, but it is still a pain (in terms of time + memory consumption).</p> <ol> <li>Store and load it using hdf5 via <a href="http://docs.h5py.org/en/stable/quick.html">h5py</a>.</li> <li>If it’s sparse, there are <a href="https://docs.scipy.org/doc/scipy/reference/sparse.linalg.html">many ways to speed it up</a>, e.g. <code class="language-plaintext highlighter-rouge">scipy.sparse.linalg.eigs(h)</code></li> <li>If it’s approximately sparse (many very small values), make it sparse (e.g. set small correlation values to 0). You’ll lose info, so you’d need to weigh the cost/benefits</li> <li>Do you really need all the eigenvalues? If not (e.g. only need the largest few or smallest few), Lanczos algorithm gives a <a href="https://scicomp.stackexchange.com/questions/23536/quality-of-eigenvalue-approximation-in-lanczos-method">much faster approximation of the tails</a>.</li> </ol>]]></content><author><name></name></author><category term="acad"/><category term="fmri"/><category term="networks"/><summary type="html"><![CDATA[In fMRI analyses, a common approach is to use a correlation matrix to study the relationships between various brain regions. Brain parcellations schemes seem to be getting finer over the years and that will increase with more detailed parcellation schemes proposed / better imaging resolution - that means there might be more and more regions.]]></summary></entry><entry><title type="html">Repurposing hard disks</title><link href="https://yihao001.github.io/blog/2019/repurpose/" rel="alternate" type="text/html" title="Repurposing hard disks"/><published>2019-10-12T16:00:00+00:00</published><updated>2019-10-12T16:00:00+00:00</updated><id>https://yihao001.github.io/blog/2019/repurpose</id><content type="html" xml:base="https://yihao001.github.io/blog/2019/repurpose/"><![CDATA[<p>tl;dr: If your desktop / laptop breaks down, it doesn’t always mean that everything is broken (it’s usually because of 1 critical component). Out of all the components, one of the useful parts that can still be used (without going through too much trouble) is your <strong>internal</strong> hard disk. You can repurpose it with a few tweaks and get additional storage space!</p> <p>Caveats: Some laptops aren’t easy to disassemble and will require <a href="https://en.wikipedia.org/wiki/Torx">special screwdrivers</a> but most of them are easy. Money spent to buy these equipments &lt; value you will unlock by reusing parts that still work. :) <br/><br/></p> <hr/> <p><br/></p> <h3 id="backstory">Backstory</h3> <p>Recently, my all-in-one desktop had issues with starting up and I accidentally killed the hard disk by switching off the power while it was booting up… it just crashes every time it tries to start (I’m guessing the disk got scratched and some parts that were essential for the booting process got corrupted - a few of my files were corrupted too) but luckily I could still access the command prompt in the Windows Recovery Environment. But <a href="https://support.microsoft.com/en-us/help/323007/how-to-copy-a-folder-to-another-folder-and-retain-its-permissions">transferring files from a hard disk to another via the command prompt (with the xcopy command) was a huge pain</a> especialy when my hard disk was bigger than my external HDD, had to transfer things to and fro another laptop multiple times.</p> <p>So I got a <a href="https://www.courts.com.sg/samsung-mz-76e500bw-2-5in-860-evo-500gb-ssd-internal-ssd-ip137598">new SSD (an affordable Samsung EVO860 500GB)</a> and got it set up pretty quickly and the desktop revived. Guess the hypothesis was quite correct.</p> <h3 id="main-content">Main content</h3> <p>I have a habit of disassembling my old/spoilt computers and sending everything except the hard disk for recycling. So this incident made me realise that they could be repurposed as an external HDD - but it took more time than expected. I did quite a lot of reading up, but still ended up buying the wrong things. But with the following information gathered from this experience, repurposing is actually as simple as:</p> <ol> <li>Disassembling your computer</li> <li>Getting the correct SATA-USB cable</li> <li>Plugging it into another computer just like what you do with a portable HDD</li> </ol> <h3 id="essential-background-knowledge">Essential background knowledge</h3> <ol> <li> <p>IDE/PATA vs SATA</p> <p>I had a range of hard disks (made in 2006, ~2012, 2016) and the 2006 was an 80GB one (from an ancient NEC laptop!)</p> <p>Here’s how to <a href="https://www.reclaime.com/library/how-to-tell-ide-from-sata.aspx">differentiate between them</a>, basically SATA has a flatter connector while IDE has 2 rows of wire ends. The latter is much older (it’s the 2006 one for mine!)</p> <p>There are 2 types of SATA: 2.5 inch and 3.5 inch, the latter is generally older than the former (and mine was both 2.5 inch HDDs).</p> </li> <li> <p>Enclosure or Cable?</p> <p>Many websites recommended <a href="https://www.howtogeek.com/268249/how-to-turn-an-old-hard-drive-into-an-external-drive/">buying an enclosure</a> but I wasn’t a fan of that idea. Instead, I bought the <a href="https://www.qoo10.sg/item/USB-3-0-TO-2-5-INCH-SATA-CABLE-CONVERTER-ADAPTER-HARD-DISK-6/603602163?banner_no=1305330">cheapest (but still legitimate looking) SATA to USB cable I’ve found</a> and it does light up but the hard disk doesn’t get recognised by the computer (and the hard disk isn’t spinning at all). Naturally, all the debugging didn’t work.</p> <p>On hindsight, I should’ve spent a few more dollars to get one with <a href="https://superuser.com/questions/1372617/can-a-sata-to-usb-cable-run-a-sata-hard-disk-off-a-usb-port">extra power supply</a>. Got one from <a href="https://www.lazada.sg/products/free-uk-power-adapterugreen-sata-to-usb-adapter-usb-30-20-cable-to-sata-converter-for-samsung-seagate-wd-25-35-hdd-ssd-hard-disk-usb-sata-adapter-uk-plug-intl-i6393383-s8040426.html">here</a> and it generally worked well. So you don’t really need an enclosure after all (if you’re okay with leaving the hard disk in the open - just remember to keep it in an airy place, wipe it when it’s dusty, or just use it only when you need to).</p> </li> </ol>]]></content><author><name></name></author><category term="non-acad"/><category term="tech"/><category term="green"/><summary type="html"><![CDATA[many parts of a crashed computer are still reusable]]></summary></entry></feed>